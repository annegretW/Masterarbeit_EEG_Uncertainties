{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Simple Gaussian Sampling\n",
    "## Overview\n",
    "The goal of this example is to demonstrate the use of MUQ's MCMC stack by sampling\n",
    "a simple bivariate Gaussian density.  To keep things as simple as possible, we\n",
    "employ a Metropolis-Hastings transition kernel with a simple random walk proposal.\n",
    "The idea is to introduce the MUQ MCMC workflow without the additional\n",
    "complexities that come from more challenging target densities or more complicated\n",
    "MCMC algorithms.\n",
    "\n",
    "### Background\n",
    "Let $x$ denote a random variable taking values in a space $\\mathcal{X}$, and let $\\pi(x)$ denote the\n",
    "probability density of $x$.  In many cases, we cannot compute expectations with\n",
    "respect to $\\pi(x)$ analytically, and we need to resort to some sort of numerical\n",
    "integration.  Typically, such approaches approximate an expectation $\\mathbb{E}_x \\left[f(x)\\right]$,\n",
    "through some sort of weighted sum that takes the form\n",
    "$$\n",
    "\\mathbb{E}_x\\left[f(x)\\right] \\approx \\sum_{k=1}^K w_k f\\left(x^{(k)}\\right).\n",
    "$$\n",
    "In standard Monte Carlo procedures, the weights are constant $w_k=\\frac{1}{K}$ and\n",
    "points $x^{(k)}$ are independent samples of $\\pi(x)$.   However, generating\n",
    "independent samples is not always possible for general $\\pi(x)$.  Markov chain\n",
    "Monte Carlo is one way to get around this.   The basic idea is to create a sequence\n",
    "of points (e.g., a chain) that are correlated, but for large $K$, can still be used in a Monte\n",
    "Carlo approximation.  We further restrict that the chain is Markov, which simply means that $x^{(k)}$\n",
    "depends only on the previous step in the chain $x^{(k-1)}$.\n",
    "\n",
    "#### Transition Kernels\n",
    "The transition from $x^{(k-1)}$ to $x^{(k)}$ is controlled by a probability distribution\n",
    "over $\\mathcal{X}$ called the transition kernel.  This distribution is consturcted\n",
    "to ensure that the chain can be used to form a Monte Carlo approximation as $K\\rightarrow \\infty$.\n",
    "More precisely, the transition kernel is constructed to ensure that the chain forms\n",
    "a stationary random process with stationary distribution $\\pi(x)$ and is ergodic,\n",
    "which ensures the sample mean will converge to the true expecation almost surely\n",
    "as $K\\rightarrow \\infty$.  Note that establishing a central limit theorem and\n",
    "studying the variance of the MCMC estimator requires additional technical conditions\n",
    "on the transition kernel.  See <a href=https://www.springer.com/us/book/9780387212395>Robert and Casella, <i>Monte Carlo Statistical Methods</i></a>\n",
    "for more details.\n",
    "\n",
    "#### Metropolis-Hastings Rule\n",
    "While not the only option, the Metropolis-Hastings rule is one of the most common\n",
    "methods for constructing an appropriate transition kernel.  The idea is to start\n",
    "with a proposal distribution $q(x | x^{(k-1)})$ and then \"correct\" the proposal\n",
    "with an accept/reject step to ensure ergodicity.  The basic procedure is as follows\n",
    "\n",
    "1. Generate a sample $x^\\prime$ from the proposal distribution\n",
    "$$\n",
    "x^\\prime \\sim q(x | x^{(k-1)}).\n",
    "$$\n",
    "\n",
    "2. Compute the acceptance probability $\\gamma$\n",
    "$$\n",
    "\\gamma = \\frac{\\pi(x^\\prime)}{\\pi(x^{(k-1)})} \\frac{q(x^{(k-1)} | x^\\prime)}{q(x^\\prime | x^{(k-1)})}.\n",
    "$$\n",
    "\n",
    "3.  Accept the proposed step $x^\\prime$ as the next state with probability $\\gamma$\n",
    "$$\n",
    "x^{(k)} = \\left\\{ \\begin{array}{lll} x^\\prime & \\text{with probability} & \\gamma\\\\ x^{(k-1)} & \\text{with probability} & 1.0-\\gamma\\end{array}\\right\\}.\n",
    "$$\n",
    "\n",
    "#### Proposal Distributions\n",
    "Clearly, the proposal density $q(x | x^{(k-1)})$ is a key component of the Metropolis-Hastings\n",
    "transition kernel.  Fortunately though, there is incredible flexibility in the\n",
    "choice of proposal; the Metropolis-Hastings correction step ensures, at least\n",
    "asymptotically, that the resulting chain will be ergodic.   While not\n",
    "the most efficient, a common choice of proposal is an isotropic Gaussian distribution\n",
    "centered at $x^{(k-1)}$.  The resulting algorithm is commonly referred to as\n",
    "the \"Random Walk Metropolis\" (RWM) algorithm.  Below, we will use the RWM\n",
    "algorithm in MUQ to sample a simple bivariate Gaussian target density $\\pi(x)$.\n",
    "\n",
    "### MCMC in MUQ\n",
    "The MCMC classes in MUQ are analogous to the mathematical components of an MCMC\n",
    "algorithm: there is a base class representing the chain, another base class representing\n",
    "the transition kernel, and for Metropolis-Hastings, a third base class representing\n",
    "the proposal distribution.  The RWM algorithm can be constructed by combining an\n",
    "instance of the \"SingleChainMCMC\" chain class, with an instance of the \"MHKernel\"\n",
    "transition kernel class, and an instance of the \"RandomWalk\" proposal class.  In\n",
    "later examples, the flexibility of this structure will become clear, as we construct\n",
    "algorithms of increasing complexity by simply exchanging each of these components\n",
    "with alternative chains, kernels, and proposals.\n",
    "\n",
    "## Problem Description\n",
    "Use the Random Walk Metropolis algorithm to sample a two-dimensional normal\n",
    "distribution with mean\n",
    "$$\n",
    "\\mu = \\left[\\begin{array}{c} 1.0\\\\\\\\ 2.0\\end{array}\\right],\n",
    "$$\n",
    "and covariance\n",
    "$$\n",
    "\\Sigma = \\left[\\begin{array}{cc} 1.0 & 0.8 \\\\\\\\ 0.8 & 1.5 \\end{array}\\right].\n",
    "$$\n",
    "\n",
    "## Implementation\n",
    "To sample the Gaussian target, the code needs to do four things:\n",
    "\n",
    "1. Define the target density and set up a sampling problem.\n",
    "\n",
    "2. Construct the RWM algorithm.\n",
    "\n",
    "3. Run the MCMC algorithm.\n",
    "\n",
    "4. Analyze the results.\n",
    "\n",
    "### Import statements\n",
    "Import MUQ python modules.\n",
    "\n",
    "Note: Ensure that the path to your MUQ libraries, e.g. /path/to/MUQ/lib, is in your $PYTHONPATH. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import muq.Modeling as mm # import MUQ modeling module\n",
    "import muq.SamplingAlgorithms as ms # import MUQ SamplingAlgorithms module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the target density and set up sampling problem\n",
    "  MUQ has extensive tools for combining many model compoenents into larger\n",
    "  more complicated models.  The AbstractSamplingProblem base class and its\n",
    "  children, like the SamplingProblem class, define the interface between\n",
    "  sampling algorithms like MCMC and the models and densities they work with.\n",
    "\n",
    "  Here, we create a very simple target density and then construct a SamplingProblem\n",
    "  directly from the density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Target Density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([1.0, 2.0]) # mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.array([[1.0,0.8],[0.8,1.5]]) # covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDensity = mm.Gaussian(mu, cov).AsDensity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Sampling Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = ms.SamplingProblem(targetDensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct the RWM algorithm\n",
    "One of the easiest ways to define an MCMC algorithm is to put all of the algorithm parameters, including the kernel and proposal definitions, in a property tree (dictionary in python) and then let MUQ construct each of the algorithm components: chain, kernel, and proposal.\n",
    "\n",
    "The dictionary will have the following entries:\n",
    "\n",
    "- NumSamples : 10000\n",
    "- KernelList \"Kernel1\"\n",
    "- Kernel1\n",
    "   * Method : \"MHKernel\"\n",
    "   * Proposal : \"MyProposal\"\n",
    "   * MyProposal\n",
    "     + Method : \"MHProposal\"\n",
    "     + ProposalVariance : 0.5\n",
    "\n",
    "At the base level, we specify the number of steps in the chain with the entry \"NumSamples\". Note that this number includes any burnin samples.   The kernel is then defined in the \"KernelList\" entry.  The value, \"Kernel1\", specifies a block in the property tree with the kernel definition.  In the \"Kernel1\" block, we set the kernel to \"MHKernel,\" which specifies that we want to use the Metropolis-Hastings kernel.  We also tell MUQ to use the \"MyProposal\" block to define the proposal. The proposal method is specified as \"MHProposal\", which is the random walk proposal used in the RWM algorithm, and the proposal variance is set to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the sampler\n",
    "\n",
    "propDef = {\n",
    "    'Method' : 'MHProposal',\n",
    "    'ProposalVariance' : 1.0\n",
    "}\n",
    "\n",
    "kernDef = {\n",
    "    'Method' : 'MHKernel',\n",
    "    'Proposal' : 'MyProposal',\n",
    "    'MyProposal' : propDef\n",
    "}\n",
    "\n",
    "chainOpts = {\n",
    "    'NumSamples' : 10000,\n",
    "    'PrintLevel' : 3,\n",
    "    'KernelList' : 'Kernel1',\n",
    "    'Kernel1' : kernDef\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the algorithm parameters are specified, we can pass them to the SingleChainMCMC constructor to create an instance of the MCMC algorithm we defined in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = ms.SingleChainMCMC(chainOpts,problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the MCMC algorithm\n",
    "  We are now ready to run the MCMC algorithm.  Here we start the chain at the\n",
    "  target densities mean.   The resulting samples are returned in an instance\n",
    "  of the SampleCollection class, which internally holds the steps in the MCMC chain\n",
    "  as a vector of weighted SamplingState's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting single chain MCMC sampler...\n",
      "  10% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 52%\n",
      "  20% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 51%\n",
      "  30% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 50%\n",
      "  40% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 50%\n",
      "  50% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 50%\n",
      "  60% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 50%\n",
      "  70% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 51%\n",
      "  80% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 51%\n",
      "  90% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 51%\n",
      "  100% Complete\n",
      "    Block 0:\n",
      "      MHKernel acceptance Rate = 51%\n",
      "Completed in 0.0377866 seconds.\n"
     ]
    }
   ],
   "source": [
    "startPt = mu\n",
    "samps = mcmc.Run([startPt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze the results\n",
    "\n",
    "  When looking at the entries in a SampleCollection, it is important to note that\n",
    "  the states stored by a SampleCollection are weighted even in the MCMC setting.\n",
    "  When a proposal $x^\\prime$ is rejected, instead of making a copy of $x^{(k-1)}$\n",
    "  for $x^{(k)}$, the weight on $x^{(k-1)}$ is simply incremented.  This is useful\n",
    "  for large chains in high dimensional parameter spaces, where storing all duplicates\n",
    "  could quickly consume available memory.\n",
    "\n",
    "  The SampleCollection class provides several functions for computing sample moments.\n",
    "  For example, here we compute the mean, variance, and third central moment.\n",
    "  While the third moment is actually a tensor, here we only return the marginal\n",
    "  values, i.e., $\\mathbb{E}_x[(x_i-\\mu_i)^3]$ for each $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean = [1.04041477 2.05063116]\n"
     ]
    }
   ],
   "source": [
    "sampMean = samps.Mean()\n",
    "print(f\"Sample Mean = {sampMean.transpose()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Variance = [0.99612687 1.4506638 ]\n"
     ]
    }
   ],
   "source": [
    "sampVar = samps.Variance()\n",
    "print(f\"Sample Variance = {sampVar.transpose()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Covariance = [[0.99612687 0.79097124]\n",
      " [0.79097124 1.4506638 ]]\n"
     ]
    }
   ],
   "source": [
    "sampCov = samps.Covariance()\n",
    "print(f\"Sample Covariance = {sampCov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Third Moment = [0.07392513 0.13543321]\n"
     ]
    }
   ],
   "source": [
    "sampMom3 = samps.CentralMoment(3)\n",
    "print(f\"Sample Third Moment = {sampMom3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Accuracy\n",
    "\n",
    "In addition to looking at moments and other expectations with respect to the target distribution, we can also look at the statistical accuracy of the mean estimate.  Specifically, we can look at the Monte Carlo Standard Error (MCSE) and effective sample size of the chain.   There are multiple ways of computing these quantities and MUQ provides implementations of both batch and spectral methods.   Batch methods use the means of different subsets of the chain to estimate the estimator variance whereas spectral methods look at the autocorrelation of the chain.   MUQ uses the spectral method described in [Monte Carlo errors with less error](https://doi.org/10.1016/S0010-4655(03)00467-3).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESS:\n",
      "  Batch:     [646.6319379  544.54418258]\n",
      "  Spectral:  [587.29522318 487.36656117]\n",
      "MCSE:\n",
      "  Batch:     [0.03924901 0.05161392]\n",
      "  Spectral:  [0.04118405 0.05455763]\n"
     ]
    }
   ],
   "source": [
    "batchESS = samps.ESS(method=\"Batch\")\n",
    "batchMCSE = samps.StandardError(method=\"Batch\")\n",
    "\n",
    "spectralESS = samps.ESS(method=\"Wolff\")\n",
    "spectralMCSE = samps.StandardError(method=\"Wolff\")\n",
    "\n",
    "print('ESS:')\n",
    "print('  Batch:    ', batchESS)\n",
    "print('  Spectral: ', spectralESS)\n",
    "\n",
    "print('MCSE:')\n",
    "print('  Batch:    ', batchMCSE)\n",
    "print('  Spectral: ', spectralMCSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ESS and MCSE quantities above are computed separately for each component of the chain.   MUQ also provides an implementation of the multivariate effective size described in [Multivariate output analysis for Markov chain Monte Carlo](https://doi.org/10.1093/biomet/asz002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate:\n",
      "  ESS:   [992.54000542]\n",
      "  MCSE:  [0.03167986 0.03823045]\n"
     ]
    }
   ],
   "source": [
    "multiESS = samps.ESS(method=\"MultiBatch\")\n",
    "multiMCSE = samps.StandardError(method=\"MultiBatch\")\n",
    "print('Multivariate:')\n",
    "print('  ESS:  ', multiESS)\n",
    "print('  MCSE: ', multiMCSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute convergence diagnostics\n",
    "To quantitatively assess whether the chain has converged, we need to run multiple\n",
    "chains and then compare the results.  Below we run 3 more independent chains (for a total of 4)\n",
    "and then analyze convergence using the commonly employed $\\hat{R}$ diagnostic.  A value of $\\hat{R}$ close to $1$ (e.g., $<1.01$) implies that the chains have converged.  More discussion on this point, as well as a description of the split-rank approach used in MUQ to estimat $\\hat{R}$, can be found in [Rank-normalization, folding, and localization: An improved R for assessing convergence of MCMC](https://arxiv.org/pdf/1903.08008.pdf).\n",
    "\n",
    "Notice that a new MCMC sampler is defined each time with a randomly selected starting point.  If we simply called `mcmc.Run()`\n",
    "multiple times, the sampler would always pick up where it left off.  For the estimation of $\\hat{R}$, it is also important that \n",
    "the initials states of these chains be drawn from a distribution that is more \"diffuse\" than the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running chain 0...\n",
      "Running chain 1...\n",
      "Running chain 2...\n",
      "Running chain 3...\n"
     ]
    }
   ],
   "source": [
    "chainOpts[\"PrintLevel\"] = 0\n",
    "numChains = 4\n",
    "\n",
    "chains = [samps]\n",
    "\n",
    "for i in range(numChains):\n",
    "    print(\"Running chain {}...\".format(i), flush=True)\n",
    "    \n",
    "    x0 = startPt + 1.5*np.random.randn(mu.shape[0])\n",
    "\n",
    "    mcmc = ms.SingleChainMCMC(chainOpts,problem)\n",
    "    chains.append( mcmc.Run([x0]) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhat =  [1.0014217  1.00125501]\n"
     ]
    }
   ],
   "source": [
    "rhat = ms.Diagnostics.Rhat(chains)\n",
    "print(\"Rhat = \", rhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
